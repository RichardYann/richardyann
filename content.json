{"meta":{"title":"ATOM 阿瞳目","subtitle":"欲穷千里目，更上一层楼","description":"欢迎光临~","author":"Richard Yann","url":"https://richardyann.gitee.io","root":"/"},"pages":[{"title":"categories","date":"2020-05-27T11:40:37.000Z","updated":"2020-05-27T21:21:28.000Z","comments":false,"path":"categories/index.html","permalink":"https://richardyann.gitee.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-05-27T11:40:10.000Z","updated":"2020-05-27T21:21:28.000Z","comments":false,"path":"tags/index.html","permalink":"https://richardyann.gitee.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2020-11-18T13:13:57.746Z","updated":"2020-05-27T21:21:28.000Z","comments":true,"path":"2020/11/18/hello-world/","link":"","permalink":"https://richardyann.gitee.io/2020/11/18/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Chrome浏览器默认全屏启动（非--kiosk模式）","slug":"Chrome浏览器默认全屏启动（非-kiosk模式）","date":"2020-05-16T06:32:13.000Z","updated":"2020-05-28T02:16:22.881Z","comments":true,"path":"2020/05/16/Chrome浏览器默认全屏启动（非-kiosk模式）/","link":"","permalink":"https://richardyann.gitee.io/2020/05/16/Chrome%E6%B5%8F%E8%A7%88%E5%99%A8%E9%BB%98%E8%AE%A4%E5%85%A8%E5%B1%8F%E5%90%AF%E5%8A%A8%EF%BC%88%E9%9D%9E-kiosk%E6%A8%A1%E5%BC%8F%EF%BC%89/","excerpt":"","text":"十几天后更新：&emsp;&emsp;现在又看了一遍前面的方法，真是太蠢了！！&emsp;&emsp;实际上，只需要修改快捷方式的参数，只不过后面不加 –kiosk，而是在后面加 –start-fullscreen 即可！ 后面的什么问题都没有了 &emsp;&emsp;也不知道之前是哪根筋儿抽了。。。 &emsp;&emsp;最近捣鼓电脑的时候，觉得chrome全屏模式使用起来视觉效果很棒，再配合一些插件和快捷键，甚至不需要动鼠标，在全屏模式下就可以流畅丝滑的浏览网页&emsp;&emsp;（全屏浏览用到的插件后面会提到） &emsp;&emsp;那么作为一个懒癌晚期患者，我自然不希望每次打开chrome后还得手动全屏，或者按F11键全屏（F11辣么远，真累T_T）。于是网上搜了一圈，结果发现所有的办法都是在快捷方式后面加 –kiosk 参数。 &emsp;&emsp;但是这个方法有个非常严重的弊端——无法退出全屏，甚至连右键都无法使用！ 这绝对是我无法忍受的！ &emsp;&emsp;因为 –kiosk参数本身是chrome自带的演示模式，用来给客户展示特定的网页，在这种情况下并不希望客户有多余的操作，但这对我们普通使用者来说就不是那么友好了，我们只是希望启动后默认全屏，正常的全屏！！ &emsp;&emsp;经过锲而不舍的尝试，我总算解决了这个问题 ↓ ↓ ↓ 总体思路利用 .bat 文件，通过命令让chrome全屏；再创建快捷方式并修改图标 step 1在桌面新建一个 txt 文档，打开后里面编辑： 12start chrome --start-fullscreenexit 保存后命名为 chrome.bat(有的电脑默认不显示扩展名，那么这样改没用，需要先显示扩展名再修改) 注意！这里的命令基本上只能这么写，如果在 “chrome” 前不加 “start” ，那么即使后面有 “exit”，打开程序后命令行黑框也无法自动关闭；如果把 “chrome” 换成 “chrome.exe”，打开后将会成为 kiosk模式 step 2将 chrome.bat 文件移动到 chrome 所在的文件夹下（必须放在这里，因为上面的命令由于各种原因只能用相对地址） step 3右键 chrome.bat 建立快捷方式到桌面，并对快捷方式右键 &gt; 属性 &gt; 更改图标，找到刚才 chrome 所在文件夹，选择 chrome.exe 就会出现 chrome的图标最后把快捷方式的名称一改就大功告成 step4如果还想固定到开始菜单 or 任务栏的话，因为是自己建的快捷方式，所以需要如下操作： 固定到开始菜单：找到开始菜单的文件夹，把快捷方式放入文件夹中 固定到任务栏：a) 在任意位置新建一个文件夹，把快捷方式放入其中，然后右键任务栏 &gt; 工具栏 &gt; 新建工具栏 &gt; 选择刚才新建的文件夹b) 然后右键新建的工具栏（一般在任务栏右边）取消勾选 “显示文本”“显示标题”c) 把新工具栏移到最左边（强行移，使劲儿移，一次移不过去，拖着它怼两下就移过去了） 最终点击图标直接打开正常的全屏模式，效果如图： 至于前面说的全屏浏览插件，就两个： tabtiles ： 可以在全屏时鼠标移动到特定位置，显示标签页和地址栏，平时隐藏（还不错，就是有点丑，有了vimium说实话其实可以不要这个了） vimium ： 可以像 vim 一样 完全用键盘进行任何操作（看着复杂，但很快就会得心应手） 具体大家可以自行搜索、安装、尝试，我就不多介绍了 然后，在chrome的扩展程序中可以设置 键盘快捷键，这样连扩展程序我们也可以在全屏下使用了 最后，记一记常用的chrome快捷键（因为在某些特定页面，比如新标签页、谷歌商店、设置页面等，上面的插件不能使用），如 Ctrl + W关闭当前标签页，Ctrl + Tab切换标签页， Alt + 左右键前进和后退","categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"https://richardyann.gitee.io/categories/%E7%9E%8E%E6%8A%98%E8%85%BE/"}],"tags":[{"name":"chrome","slug":"chrome","permalink":"https://richardyann.gitee.io/tags/chrome/"}]},{"title":"高斯混合模型GMM聚类的步骤和推导","slug":"高斯混合模型GMM聚类的步骤和推导","date":"2020-03-20T07:18:57.000Z","updated":"2020-05-28T03:15:15.492Z","comments":true,"path":"2020/03/20/高斯混合模型GMM聚类的步骤和推导/","link":"","permalink":"https://richardyann.gitee.io/2020/03/20/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8BGMM%E8%81%9A%E7%B1%BB%E7%9A%84%E6%AD%A5%E9%AA%A4%E5%92%8C%E6%8E%A8%E5%AF%BC/","excerpt":"","text":"0. 引言&emsp;&emsp;由于最近要做聚类算法方面的内容，看了很多资料，在高斯混合模型(GMM)这里一直没有一个让我完全推导清楚的、理解的文章。经过三天打鱼两天晒网 不懈努力，总算是有一点自己的理解，我希望尽量通俗地把GMM讲明白，同时也希望尽量详细地对公式进行推导和解释。因此，我会先给出GMM算法的总体步骤，保证拿上先可以直接使用，然后再进行具体的推导和解释。文中可能有一些自己理解不严谨的地方，还请大家指正。 1. 算法初窥 &emsp;&emsp;已知样本集是$D={x_1,x_2,…,x_m}$，要将这些样本聚成 $k$ 类。我们认为样本服从混合高斯分布：$$p_M(\\pmb{x})=\\sum_{i=1}^k \\alpha_i \\cdot p(\\pmb{x}|\\pmb{\\mu_i}, \\pmb{\\Sigma_i})$$&emsp;&emsp;其中$p(\\pmb{x}|\\pmb{\\mu_i}, \\pmb{\\Sigma_i})=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\pmb{\\Sigma_i}|^{\\frac{1}{2}}}exp{-\\frac{1}{2}(\\pmb{x}-\\pmb{\\mu_i})^T\\pmb{\\Sigma_i}^{-1}(\\pmb{x}-\\pmb{\\mu_i})}$是一个多元高斯分布，即一个混合成分；&emsp;&emsp;$\\alpha_i$表示混合系数，即选择第 $i$ 个混合成分的概率。 第一步 初始化高斯混合分布的模型参数 $\\alpha_i,\\pmb{\\mu_i},\\pmb{\\Sigma_i}$第二步 计算$x_j$由各混合成分生成的后验概率，即观测数据$x_j$由第 $i$ 个分模型生成的概率$p(z_j=i|\\pmb{x_j})$，并记为$\\gamma_{ji}$&emsp;&emsp;&emsp;&emsp;$\\gamma_{ji}=\\frac{\\alpha_i\\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})}{\\sum_{l=1}^{k}\\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})}$第三步 计算新的模型参数：&emsp;&emsp;&emsp;&emsp;$\\pmb{\\mu_i’}=\\frac{\\Sigma_{j=1}^m\\gamma_{ji}\\pmb{x_j}}{\\Sigma_{j=1}^m\\gamma_{ji}}$&emsp;&emsp;&emsp;&emsp;$\\pmb{\\Sigma_i’}=\\frac{\\Sigma_{j=1}^m\\gamma_{ji}(\\pmb{x_j}-\\pmb{\\mu_i’})(\\pmb{x_j}-\\pmb{\\mu_i’})^T}{\\Sigma_{j=1}^m\\gamma_{ji}}$&emsp;&emsp;&emsp;&emsp;$\\alpha_i’=\\frac{\\Sigma_{j=1}^m\\gamma_{ji}}{m}$第四步 按照新的模型参数重复2，3步，直到满足停止条件第五步 将每个样本按照$\\lambda_j=\\arg\\max\\limits_{i\\in{1,2,…,k}} \\gamma_{ji}$划入对应的簇。即对每个样本来自哪个分模型的概率大就划入哪个分模型的簇中，最终就得到了 $k$ 个聚类 2. 高斯混合模型的引入&emsp;&emsp;与k-means聚类不同，高斯混合聚类是采用概率模型来刻画聚类结构。实际上我们可以采用任意不同的概率分布模型来进行刻画，高斯分布是最普遍的一种，如下：&emsp;&emsp;高斯分布：$$p(\\pmb{x})=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\pmb{\\Sigma}|^{\\frac{1}{2}}}exp[-\\frac{1}{2}(\\pmb{x}-\\pmb{\\mu})^T\\pmb{\\Sigma}^{-1}(\\pmb{x}-\\pmb{\\mu})]$$&emsp;&emsp;而单高斯分布模型有其局限性，不能完全反映数据分布的特点，因此我们用多个高斯分布的线性叠加来刻画实际样本，其中一个高斯分模型称为一个混合成分。 理论上来说，当叠加的高斯分模型数量足够多时，可以表征任意一种分布。（这其实很好理解，类比足够多微小线段可以逼近任意一条曲线、足够多复指数信号可以描述任意信号…是一样的道理） &emsp;&emsp;高斯混合分布：$$p_M(\\pmb{x})=\\sum_{i=1}^k \\alpha_i \\cdot p(\\pmb{x}|\\pmb{\\mu_i}, \\pmb{\\Sigma_i})$$我们认为，手里拿到的样本就是根据这个概率分布抽取得到的（或者说“生成的”）例如，对于第$j$个样本$\\pmb{x_j}$，就根据 $p_M(\\pmb{x_j})=\\sum_{i=1}^k \\alpha_{ji} \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i}, \\pmb{\\Sigma_i})$ 得到。 $\\pmb{TIPS:}$ 这里的$p(x)$、$p_M(x)$指的是概率密度函数，不是概率，在有些概率书上为了区别，用$f(x)$表示，这里都用$p(x)$表示，但心里要清楚其含义。 接上条，所以$p(\\pmb{x}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})$不是条件概率，而是概率密度，”$|\\mu_i,\\Sigma_i$”只是明确一下这个概率密度函数包含的参变量。实际上它表示的就是上面单高斯分布的$p(x)$。 $\\pmb{x}$是一条样本，但是有$n$个维度，因此是一个$n$维向量。 $\\alpha_i&gt;0$是在生成这条样本时，选择通过第$i$个分模型来生成的概率，且$\\sum_{i=1}^k\\alpha_i=1$。（不能说成”样本来自第$i$个分模型的概率”，因为这里是一个先验的情况，如果这样说就成了后验了） $\\pmb{\\mu}_i,\\pmb{\\Sigma}_i$是第$i$个分模型的参数。其中，$\\pmb{\\mu}_i$表示均值，是一个$n$维向量，$\\pmb{\\Sigma}_i$表示协方差矩阵，是一个$n×n$方阵。 3. 按照高斯混合模型进行聚类划分&emsp;&emsp;上面说了我们认为手里拿到的样本就是通过高斯混合模型抽取得到的，那么反过来我们要怎么把这些样本用高斯混合模型划分成不同的类别呢？&emsp;&emsp;一个很直接的想法自然是按照模型的混合成分划成 $k$ 类，一个数据最可能从哪个分模型得来就认为属于哪一类。&emsp;&emsp;在这里，我们要引入一个隐变量 $z_j\\in{1,2,…,k}$ 表示得到样本 $\\pmb{x_j}$的高斯分模型。 注： 有的书上用一维向量来表示，即若认为样本$\\pmb{x_j}$来自第2个高斯分模型，则$z_j=[0,1,0,0,…,0]$。 这里直接用数字来表示来自第几个分模型。 根据$z_j$的含义很容易看出，$P(z_j=i)$表示$\\pmb{x_j}$是通过第$i$个分模型生成的概率，就是高斯混合模型中的参数$\\alpha_{ji}$ &emsp;&emsp;前面我们说了，$\\alpha$是一个先验概念，是从模型到样本的过程。而现在我们在已经拿到了样本的情况下反推其来自哪个分模型，是逆向过程，因此我们用$p_M(z_j=i|\\pmb{x_j})$来表示样本$\\pmb{x_j}$来自第$i$个分模型的后验概率，并简记为$\\gamma_{ji}$。有：$$p_M(z_j=i|\\pmb{x_j})=\\frac{P(z_j=i)\\cdot p_M(\\pmb{x_j}|z_j=i)}{p_M(\\pmb{x_j})} \\\\ =\\frac{\\alpha_i\\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})}{\\sum_{l=1}^{k}\\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})}$$ 注： $p_M(\\pmb{x_j}|z_j=i)$表示按照第$i$个高斯分模型生成$\\pmb{x_j}$的概率密度，第$i$个高斯分模型的参数是$\\pmb{\\mu_i},\\pmb{\\Sigma_i}$，故而就等于$p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})$ $p_M(\\pmb{x_j})$表示综合所有的混合成分后总的概率密度 上述等式第一行由贝叶斯公式得到&emsp;贝叶斯公式： $p(A|B)=\\frac{p(A)p(B|A)}{p(B)}$ 那么显而易见地，每个样本$\\pmb{x_j}$的簇标记$\\lambda_j$如下确定：$$\\lambda_j=\\arg\\max_{i \\in {1,2,…,k}}\\gamma_{ji}$$即，$\\pmb{x_j}$来自哪个分模型的概率最大，就认为属于哪一类。 4. 确定高斯混合模型参数&emsp;&emsp;上面已经说了当已知高斯混合模型时，就可以进行聚类的划分，那么如何求解这个模型，得到它的三个参数呢？&emsp;&emsp;我们在这里要用到的是EM算法（期望最大算法）。其实原理很简单：为什么我们能拿到手中的样本，而不是其他数据呢？我们认为这是由于选出这样一组样本的概率最大，所以才运气爆表，被我们拿到手。&emsp;&emsp;由上文知，按照高斯混合模型选出一个样本$\\pmb{x_j}$的概率密度$$p_M(\\pmb{x_j})=\\sum_{i=1}^k\\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})$$&emsp;&emsp;对于手中的$m$个样本，选到任意一个都是一个独立事件，最终的概率自然是全部相乘，即$$\\prod_{j=1}^mp_M(\\pmb{x_j})$$但是，连乘不好处理，因此一般习惯对它取对数，于是样本集$D$的最大化对数似然函数就定义如下：$$LL(D)=\\ln(\\prod_{j=1}^mp_M(\\pmb{x_j}))\\\\ =\\sum_{j=1}^m \\ln(p_M(\\pmb{x_j}))\\\\ =\\sum_{j=1}^m \\ln(\\sum_{i=1}^k \\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i}))$$&emsp;&emsp;只要能求出使$LL(D)$最大的参数就可以了。&emsp;&emsp;那么怎么求满足要求的参数呢？&emsp;&emsp;我们设参数$\\theta_i={(\\alpha_i,\\mu_i,\\Sigma_i)}$能使$LL(D)$最大化，那么$LL(D)$对每个参数的偏导数应该为0，但是偏导数为0求出的参数有可能只是局部最优解（$LL(D)$取极大值或驻点），而不是全局最优解（$LL(D)$取最大值）。&emsp;&emsp;经过后面的推导，我们可以发现求出的每个参数，都可以用$\\gamma_{ji}$表示。所以，我们在求出了一组模型参数后，按照这种模型得到对应的$\\gamma_{ji}$，再用得到的$\\gamma_{ji}$继续按照偏导数为0的方式求出新的参数。如此循环迭代，直到我们认为足够为止。 至于为什么每次迭代都可以使求得的参数更优，这个问题就不在本文展开叙述了，具体可以参考EM算法的相关资料。有时间我会专门写一篇关于EM算法的文章。 &emsp;&emsp;现在我们来具体求解每个参数： ① $\\pmb{\\mu}:$$$\\frac{\\partial LL(D)}{\\partial \\pmb{\\mu_i}}=0\\\\ → \\frac{\\partial}{\\partial \\pmb{\\mu_i}}\\sum_{j=1}^m \\ln (\\sum_{i=1}^k \\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i}))=0\\\\ → \\sum_{j=1}^m\\frac{1}{\\sum_{l=1}^k \\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})} \\cdot \\frac{\\partial}{\\partial \\pmb{\\mu_i}}[\\sum_{l=1}^k\\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})]=0$$&emsp;&emsp;(这里因为对$\\pmb{\\mu_i}$求偏导，为了避免混淆，将求和变量写成 $l$)&emsp;&emsp;对$\\frac{\\partial}{\\partial \\pmb{\\mu_i}}[\\sum_{l=1}^k\\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})]$来说，只有当 $l=i$时，包含$\\mu_i$的内容，其余对$\\mu_i$求偏导均为0，可以舍去，则继续推导如下：$$→ \\sum_{j=1}^m\\frac{1}{\\sum_{l=1}^k \\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})} \\cdot \\frac{\\partial}{\\partial \\pmb{\\mu_i}}[ \\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})]=0$$&emsp;&emsp;其中，$$\\frac{\\partial}{\\partial \\pmb{\\mu_i}}[ \\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})]\\\\ =\\frac{\\partial}{\\partial \\pmb{\\mu_i}}{\\alpha_i\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\pmb{\\Sigma_i}|} \\exp[-\\frac{1}{2}(\\pmb{x_j}-\\pmb{\\mu_i})^T\\pmb{\\Sigma_i}^{-1}(\\pmb{x_j}-\\pmb{\\mu_i})]}\\\\ =\\alpha_i\\frac{\\exp[-\\frac{1}{2}(\\pmb{x_j}-\\pmb{\\mu_i})^T\\pmb{\\Sigma_i}^{-1}(\\pmb{x_j}-\\pmb{\\mu_i})]}{(2\\pi)^{\\frac{n}{2}}|\\pmb{\\Sigma_i}|} \\frac{\\partial}{\\partial\\pmb{\\mu_i}}[-\\frac{1}{2}(\\pmb{x_j}-\\pmb{\\mu_i})^T\\pmb{\\Sigma_i}^{-1}(\\pmb{x_j}-\\pmb{\\mu_i})]\\\\ =\\alpha_i\\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})\\cdot(\\pmb{x_j}-\\pmb{\\mu_i})$$ 这里是向量/矩阵对另一个向量求导，不是标量求导，具体可以参考矩阵求导相关资料 &emsp;&emsp;因此继续推导如下：$$→ \\sum_{j=1}^m\\frac{\\alpha_i \\cdot p(\\pmb{x_j}|\\pmb{\\mu_i},\\pmb{\\Sigma_i})}{\\sum_{l=1}^k \\alpha_l \\cdot p(\\pmb{x_j}|\\pmb{\\mu_l},\\pmb{\\Sigma_l})} (\\pmb{x_j}-\\pmb{\\mu_i})=0\\\\ → \\sum_{j=1}^m p_M(z_j=i|\\pmb{x_j})(\\pmb{x_j}-\\pmb{\\mu_i})=0\\\\ → \\sum_{j=1}^m \\gamma_{ji}\\cdot (\\pmb{x_j}-\\pmb{\\mu_i})=0\\\\ → \\sum_{j=1}^m \\gamma_{ji}\\pmb{x_j}=\\sum_{j=1}^m\\gamma_{ji}\\pmb{\\mu_i}\\\\ → \\pmb{\\mu_i}=\\frac{\\sum_{j=1}^m \\gamma_{ji}\\pmb{x_j}}{\\sum_{j=1}^m\\gamma_{ji}}$$ &emsp;&emsp;至此，参数$\\pmb{\\mu_i}$迭代公式得到。 ② $\\pmb{\\Sigma}:$&emsp;&emsp;同理，由$$\\frac{\\partial LL(D)}{\\partial \\pmb{\\Sigma_i}}=0$$&emsp;&emsp;推得：$$\\pmb{\\Sigma_i}=\\frac{\\sum_{j=1}^m\\gamma_{ji}(\\pmb{x_j}-\\pmb{\\mu_i})(\\pmb{x_j}-\\pmb{\\mu_i})^T}{\\sum_{j=1}^m\\gamma_{ji}}$$ ③ $\\alpha:$&emsp;&emsp;求$\\alpha$的过程略有不同，因为除了要使$LL(D)$最大化以外，$\\alpha$还要满足它自身的条件：$\\alpha_i ≥0,\\sum_{i=1}^k\\alpha_i=1$。&emsp;&emsp;这是一个有条件的极值问题，我们要用拉格朗日乘数法来求解（具体可以参考拉格朗日乘数法求极值的相关资料）&emsp;&emsp;相当于将$LL(D)$求极值问题转化为$LL(D)+\\lambda(\\sum_{i=1}^k\\alpha_i -1)$求极值的问题，然后依然对$\\alpha_i$求导为0，由此得到：$$\\alpha_i=\\frac{1}{m}\\sum_{j=1}^m\\gamma_{ji}$$ &emsp;&emsp;至此，高斯混合模型聚类的所有参数公式均已得到，下来只要不断迭代，并按照文章第3节中的划分方式来进行聚类划分即可。 &emsp;&emsp;最后，可以再回头看看文章第1节的算法总结。 参考资料 周志华，机器学习，清华大学出版社，2016 李航，统计学习方法，清华大学出版社，2012","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据","slug":"大数据","permalink":"https://richardyann.gitee.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据分析","slug":"数据分析","permalink":"https://richardyann.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"聚类","slug":"聚类","permalink":"https://richardyann.gitee.io/tags/%E8%81%9A%E7%B1%BB/"}]},{"title":"PCA降维的数学理解与举例","slug":"PCA降维的数学理解与举例","date":"2020-02-16T15:58:39.000Z","updated":"2020-05-28T01:23:37.331Z","comments":true,"path":"2020/02/16/PCA降维的数学理解与举例/","link":"","permalink":"https://richardyann.gitee.io/2020/02/16/PCA%E9%99%8D%E7%BB%B4%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3%E4%B8%8E%E4%B8%BE%E4%BE%8B/","excerpt":"","text":"PCA(Principle Component Analysis 主成分分析)是深度学习中最常用的降维算法。本文将通过最基础的线性代数知识对PCA算法进行解释。 0. 为什么要进行降维&amp;降维的目标&amp;降维的原则0.1 为什么要进行降维在深度学习中，需要对大量的样本数据进行处理，而每个样本会包含很多特征（即维度），这样在进行各种运算和训练时无疑会消耗大量的内存和时间，所以我们希望适当的减少每个样本的维度，从而简化运算。 0.2 降维的目标假设原始的数据集为$X_{m×n}$，如下所示：$$X_{m×n}=\\left[\\begin{matrix}x_1^{(1)}&amp;x_2^{(1)}&amp;\\dots &amp; x_n^{(1)} \\\\ x_1^{(2)}&amp;x_2^{(2)}&amp;\\dots &amp; x_n^{(2)} \\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots \\\\ x_1^{(m)}&amp; x_2^{(m)}&amp;\\dots &amp; x_n^{(m)}\\end{matrix}\\right]$$表示有m个样本数据，每个样本数据有n个维度。即上述矩阵每一行是一个样本，共m个样本；每一列是一个维度，共n个维度。 我们的目标是不改变样本数量，减少样本维度，即减少上述矩阵的列数。 假设降成k维，则最终得到的降维后数据集为：$$Y_{m×k}=\\left[\\begin{matrix}y_1^{(1)}&amp;y_2^{(1)}&amp;\\dots &amp; y_k^{(1)}\\\\ y_1^{(2)}&amp;y_2^{(2)}&amp;\\dots &amp; y_k^{(2)}\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ y_1^{(m)}&amp;y_2^{(m)}&amp;\\dots &amp; y_k^{(m)}\\end{matrix}\\right]$$ 设$Y_{m×k}=X_{m×n}Q_{n×k}$，我们只要求出Q，就可以把X降维成Y，因此我们的目标就是找出一个恰当的Q 0.3 降维的原则我们要在降低维度时尽可能减少数据的损失，所以我们让不同维度之间尽可能接近，同一维度的不同数据尽可能分散。（不同维度之间越接近，就意味着去掉一些维度时损失较小；同一维度的不同数据越分散，就意味着在这一维度上的数据越容易被区分） 我们在协方差矩阵中来衡量这一接近、分散程度。 以m个3维数据为例：（注意：a,b,c 表示不同维度，每一行是一个样本，每一列是一个维度）$$\\left[\\begin{matrix}a_1&amp;b_1&amp;c_1\\\\ a_2&amp;b_2&amp;c_2\\\\ \\vdots&amp;\\vdots&amp;\\vdots\\\\ a_m&amp;b_m&amp;c_m\\end{matrix}\\right]$$其协方差为：$$cov=\\left[\\begin{matrix}cov(a,a)&amp;cov(a,b)&amp;cov(a,c)\\\\ cov(b,a)&amp;cov(b,b)&amp;cov(b,c)\\\\ cov(c,a)&amp;cov(c,b)&amp;cov(c,c)\\end{matrix}\\right]$$容易看出在协方差矩阵中，对角线元素表示的是每个维度上样本的方差，而非对角线元素表示的是不同维度之间样本的协方差。要使得“不同维度之间尽可能接近，同一维度数据尽可能分散”，就要让非对角线元素尽可能小，对角线元素尽可能大。 我们通过对数据的处理，使其协方差矩阵成为一个对角阵，那么非对角线元素为0，我们就可以放心的删去多余的维度；在删去时，我们删掉较小的对角线元素对应的列。 1. 计算X，Y的协方差矩阵对于原始数据集$X_{m×n}$，其协方差矩阵为：$$Cx_{n×n}=\\frac{1}{m}X^TX$$（概率论里除以m-1的公式是无偏估计，但是这里数据总数已知，不涉及估计问题，只涉及离散程度的描述，因此除以m） 我们设最终数据集$Y_{m×k}=X_{m×n}Q_{n×k}$，则其协方差矩阵满足：$$Cy_{k×k}=\\frac{1}{m}Y^TY \\\\ =\\frac{1}{m}Q^TX^TXQ \\\\ =Q^TCxQ$$ 2. 对角化在降维之前，我们要处理数据集使其协方差矩阵成为对角阵，（即数据集要经过两步处理，X→ Y’ →Y），设 $Y’_{m×n}=X_{m×n}Q’_{n×n}$则同理可得 $Y’$ 的协方差矩阵为： $$Cy’_{k×k} = \\frac{1}{m}Y’^TY’ \\\\ =\\frac{1}{m}Q’^TX^TXQ’ \\\\ =Q’^TCxQ’$$ 对 $Cx$，有 $Cx=P\\Lambda P^{T}$ 设$Cx$特征值为：$\\lambda_1, \\lambda_2, \\dots, \\lambda_n$（$\\lambda_1 ≥\\lambda_2≥\\dots≥\\lambda_n$）对应特征向量为：$\\xi_1, \\xi_2, \\dots, \\xi_n$则令$P=[\\xi_1,\\xi_2,\\dots, \\xi_n]$，有$Cx=P\\Lambda P^{-1}$其中，$\\Lambda=diag[\\lambda_1, \\lambda_2, \\dots, \\lambda_n]$，是一个由特征值构成的对角阵由于协方差矩阵一定是对称阵，根据对称阵的性质，不同特征值对应的特征向量正交，即$P^TP=I=P^{-1}P$，即$P^T=P^{-1}$故有$Cx=P\\Lambda P^T$ 因此我们令$Q’=P$，则有：$$Cy’=Q’^TCxQ’ \\\\ =P^TP\\Lambda P^TP \\\\ =\\Lambda=diag[\\lambda_1 \\lambda_2 \\dots \\lambda_n]$$ 这样我们就将协方差矩阵转成了对角阵，实现这一变化的矩阵$Q’=P$。 3. 降维假设降低成k维，由于之前已经将特征值从大到小排序，所以取前k个特征值$\\lambda$对应的特征向量$\\xi$构成$Q$:$$Q=[\\xi_1,\\xi_2,\\dots,\\xi_k]$$这样得到最终数据集$Y$的协方差矩阵：$$Cy=Q^TCxQ \\\\ =diag[\\lambda_1 \\dots \\lambda_k] \\\\ (\\lambda_i是n维列向量)$$则$Y=XQ$，实现了从$X_{m×n}$到$Y_{m×k}$的降维。 4. 举例若给定原始数据集为$$X=\\left[\\begin{matrix}1&amp;-1&amp;3&amp;2&amp;0 \\\\ -2&amp;0&amp;4&amp;1&amp;1\\end{matrix}\\right]$$则其协方差矩阵为$$Cx=\\frac{1}{2}X^TX \\\\ =\\left[\\begin{matrix}2.5&amp;-0.5&amp;-2.5&amp;0&amp;-1 \\\\ -0.5&amp;0.5&amp;-1.5&amp;-1&amp;0 \\\\ -2.5&amp;-1.5&amp;12.5&amp;5&amp;2 \\\\ 0&amp;-1&amp;5&amp;2.5&amp;0.5 \\\\ -1&amp;0&amp;2&amp;0.5&amp;0.5\\end{matrix}\\right]$$其特征值为：$\\lambda_1=15.5,\\lambda_2=3,\\lambda_{3,4,5}≈0$对应特征向量$\\xi_1$~$\\xi_5$构成矩阵$P$为：$$P=[\\begin{matrix}\\xi_1&amp;\\xi_2&amp;\\xi_3&amp;\\xi_4&amp;\\xi_5]\\end{matrix} \\\\ =\\left[\\begin{matrix}-0.18&amp;-0.82&amp;-0.29&amp;0.38&amp;0.27 \\\\ -0.11&amp;0.33&amp;0.11&amp;0.90&amp;-0.23 \\\\ 0.90&amp; 0 &amp;-0.41&amp;0.15&amp;-0.05 \\\\ 0.36 &amp; -0.41 &amp; 0.82 &amp; 0.04&amp; -0.18 \\\\ 0.14 &amp; 0.24 &amp; 0.25&amp; 0.13 &amp; 0.92\\end{matrix}\\right]$$观察特征值，我们很容易发现后面三个非常小接近0，故而可以忽略，所以我们把原来的5维降低成2维是合理的，那么我们只需取最大的两个特征值对应的特征向量来构成$Q$：$$Q=[\\begin{matrix}\\xi_1&amp;\\xi_2]\\end{matrix} \\\\ =\\left[\\begin{matrix}-0.18&amp;-0.82 \\\\ -0.11&amp;0.33 \\\\ 0.90&amp; 0 \\\\ 0.36 &amp; -0.41 \\\\ 0.14 &amp; 0.24\\end{matrix}\\right]$$则得到降维后的数据集$Y:$$$Y=XQ \\\\ =\\left[\\begin{matrix}3.34&amp;-1.96 \\\\ 4.45&amp;1.47\\end{matrix}\\right]$$ 通常情况下，用PCA降维的同时还伴随着数据中心化，即使得所有数据点的中心归一到坐标原点。为与普遍的算法结果保持一致，我们对上述降维后的数据集进行中心化，即每一列减去该列的均值，得到：$$Y_{中心化}=\\left[\\begin{matrix}-0.56&amp;-1.71 \\\\ 0.56&amp;1.71\\end{matrix}\\right]$$ 在使用PCA时注意： 一定要弄清楚数据集的行、列，哪个是样本数，哪个是特征数（维度） 若数据集以行表示维度，列表示样本数（即与本文相反），那么协方差矩阵一定不能用自带的函数计算，其计算公式改为$Cx=\\frac{1}{m}XX^T$, 其他的将相应的列操作变成行操作即可。不过与其如此，不如直接把原始数据集转置。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"降维","slug":"降维","permalink":"https://richardyann.gitee.io/tags/%E9%99%8D%E7%BB%B4/"}]}],"categories":[{"name":"瞎折腾","slug":"瞎折腾","permalink":"https://richardyann.gitee.io/categories/%E7%9E%8E%E6%8A%98%E8%85%BE/"},{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"chrome","slug":"chrome","permalink":"https://richardyann.gitee.io/tags/chrome/"},{"name":"机器学习","slug":"机器学习","permalink":"https://richardyann.gitee.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据","slug":"大数据","permalink":"https://richardyann.gitee.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据分析","slug":"数据分析","permalink":"https://richardyann.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"聚类","slug":"聚类","permalink":"https://richardyann.gitee.io/tags/%E8%81%9A%E7%B1%BB/"},{"name":"降维","slug":"降维","permalink":"https://richardyann.gitee.io/tags/%E9%99%8D%E7%BB%B4/"}]}